# **LLM-Assisted Engine Health Index Estimation and Remaining Useful Life (RUL) Prediction Using NASA C-MAPSS**

## **1. Abstract**

This project presents a hybrid **Machine Learning + Large Language Model (LLM)** predictive maintenance system for turbofan engines using the **NASA C-MAPSS** dataset. A deep learning architectureâ€”combining **Temporal Convolutional Networks (TCN)**, **BiLSTM**, and **Dual Attention**â€”jointly predicts **Remaining Useful Life (RUL)** and a normalized **Engine Health Index (HI)**.

To enhance interpretability, the system integrates an offline LLM (**DeepSeek-R1 via Ollama**) that generates a structured diagnostic report summarizing:

- Sensor anomalies  
- Health degradation patterns  
- Possible failure modes  
- Maintenance recommendations  

This end-to-end pipeline provides both **quantitative predictions** and **qualitative diagnostic reasoning**, closely aligning with real-world aerospace prognostics.

---

## **2. Introduction**

Predicting turbofan engine degradation is crucial for maintenance optimization, operational safety, and cost reduction. Traditional RUL models often focus only on numerical outputs, leaving maintenance teams without interpretable explanations.

This project integrates:

- A **multitask deep learning model** that predicts both *RUL* and *HI*  
- **Attention mechanisms** to reveal critical time steps and sensor contributions  
- An **LLM-based interpreter** that transforms numerical results into readable diagnostic insights  

The combination of data-driven modeling + LLM reasoning offers a modern solution for **explainable predictive maintenance**.

---
## **Project Structure**

The project directory is organized as follows:

```
LLM-Assisted_RUL_Project/
â”‚
â”œâ”€â”€ data/(raw data)
â”‚   â”œâ”€â”€ FD001/
â”‚   â”œâ”€â”€ FD002/
â”‚   â”œâ”€â”€ FD003/
â”‚   â””â”€â”€ FD004/
â”‚   
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€__init__.py
|   |
â”‚   â”œâ”€â”€ data/(We run code using the terminal)
â”‚   â”‚   â”œâ”€â”€ data_loading.py
â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â””â”€â”€ preprocessing.py
â”‚   |
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ tcn_bilstm_dual_attn.py       â† MAIN MODEL (HI + RUL) -- for our project evaluation
â”‚   â”‚   â””â”€â”€ bilstm_baseline.py            â† BASELINE MODEL 
â”‚   |
â”‚   â”œâ”€â”€ training/(We run code using the terminal)
â”‚   â”‚   â”œâ”€â”€ train_hi_rul.py               â† TRAIN ALL FD001â€“FD004 
â”‚   â”‚   â”œâ”€â”€ train_hi_rul.py               â† TRAIN ALL FD001â€“FD004
â”‚   â”‚   â””â”€â”€ eval_rul.py                   â† EVALUATION MODULE
â”‚   |
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ llm_reasoning_ollama.py       â† (DeepSeek-r1)
â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   |
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ metrics.py
â”‚       â”œâ”€â”€ plots.py                      â† attention + RUL curves
|       â””â”€â”€ docx_report.py
|                  
â”‚   
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ preprocessed/
â”‚                                                
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## **3. NASA C-MAPSS Dataset**

### **3.1 Dataset Origin**

The **Commercial Modular Aero-Propulsion System Simulation (C-MAPSS)** dataset was released by NASA for the **PHM 2008 Challenge**, simulating realistic turbofan engine degradation under variable operational conditions.

### **3.2 Data Structure**

Each file contains:

- **unit** â€“ engine ID  
- **cycle** â€“ time step  
- **op1â€“op3** â€“ operational settings  
- **s1â€“s21** â€“ sensor measurements  
- Run-to-failure trajectories until system degradation triggers failure  

### **3.3 Subset Characteristics**

| Subset | Operating Conditions | Fault Modes | Difficulty |
|--------|----------------------|-------------|------------|
| FD001  | Single               | Single      | Easy       |
| FD002  | Multiple             | Single      | Medium     |
| FD003  | Single               | Multiple    | Medium     |
| FD004  | Multiple             | Multiple    | Hardest    |

---
## **4. System Architecture**

### **4.1 End-to-End Pipeline Overview**

### **4.1 Full Pipeline Diagram**

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚      Raw NASA Data       â”‚
      â”‚   (FD001â€“FD004 .txt)     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    Preprocessing Layer   â”‚
      â”‚  - RUL reconstruction    â”‚
      â”‚  - HI normalization      â”‚
      â”‚  - Scaling               â”‚
      â”‚  - Sliding windows       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Multitask Deep Learning  â”‚
      â”‚  TCN â†’ BiLSTM â†’ DualAttn â”‚
      â”‚ Outputs: RUL, HI, Attn   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    Evaluation Module     â”‚
      â”‚ RMSE | MAE | PHM | RÂ²    â”‚
      â”‚ + Diagnostic Plots       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   LLM Reasoning (Ollama) â”‚
      â”‚ Structured Diagnostics   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   DOCX Maintenance Reportâ”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## **Setup Instructions**

To run the project, follow these steps:

1. **Download and unzip the project folder**:
   Extract the zipped folder to your local machine. The dataset is already included in the `data/` directory.

2. **Create and activate a virtual environment**:
   Open VScode Terminal(Powershell).
   - Create a virtual environment by running:
     ```bash
     python -m venv .venv
     ```
   - **Activate the virtual environment**:
     - For **PowerShell** (VS Code default):
       ```bash
       .venv\Scripts\Activate.ps1
       ```

3. **Install the required dependencies**:
   Once the virtual environment is activated, install the necessary packages:
   ```bash
   pip install -r requirements.txt
   ```

## **5. Methodology**

### **5.1 Preprocessing**

Implemented in: `src/data/preprocessing.py`

```bash
python -m src.data.preprocessing
```

Steps include:

- Compute **true RUL** by reverse indexing  
- **Clip RUL** to stabilize training  
- Compute **HI = 1 â€“ (RUL / max_RUL)**  
- Select top-performing sensors  
- Standard scaling  
- Sliding window generation  

Output directory:

`outputs/preprocessed/FD00X/`


---

## **6. Model Architectures**

### **6.1 Baseline Model: BiLSTM RUL Predictor**

Implemented in: `src/models/bilstm_baseline.py`

Characteristics:

- 2-layer BiLSTM  
- Fully connected regression head  
- Predicts **RUL only**  
- No attention  
- No HI prediction  
- No LLM report integration  

Used mainly as a performance benchmark.

---

### **6.2 Multitask Model: TCN â†’ BiLSTM â†’ Dual Attention**

Implemented in: `src/models/tcn_bilstm_dual_attn.py`

Architecture:

### Multitask TCNâ€“BiLSTM Dual-Attention Architecture

                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      Input Sequence      â”‚
                 â”‚          (T Ã— D)         â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Temporal Convolution   â”‚
                 â”‚       (TCN Layers)       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚       BiLSTM Encoder     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                             â”‚
                â–¼                             â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚Temporal Attentionâ”‚        â”‚ Spatial Attentionâ”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                      â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    Shared Representation â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                  â–¼                      â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  RUL Head  â”‚     â”‚  HI Head   â”‚       â”‚Attention Weightsâ”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Advantages:

- TCN captures long-range temporal context  
- BiLSTM models sequential degradation  
- Dual Attention provides interpretability  
- Multitask learning improves representation quality  

Outputs:

- Remaining Useful Life (RUL)  
- Health Index (HI)  
- Attention weights  

---

## **7. LLM-Assisted Diagnostic Interpretation**

Implemented in:

- `src/llm/llm_reasoning_ollama.py`
- `src/llm/prompts.py`

Process:

1. Identify worst-performing sample  
2. Extract:
   - Sensor deviations  
   - Attention weights  
   - RUL + HI information  
3. Build structured prompt  
4. Generate inference using DeepSeek-R1 (via Ollama)  
5. Convert into readable, formatted **DOCX report** using `docx_export.py`  

Report includes:

- Health summary  
- Sensor deviations  
- Fault interpretation  
- Maintenance suggestions  

---

# **7. Training & Evaluation**

---

## **7.1 Training the Multitask Model**

To train the multitask **TCNâ€“BiLSTM Dual-Attention** model on all subsets (FD001â€“FD004), run:

```bash
python -m src.training.train_hi_rul
```

This trains the model sequentially on all four NASA subsets and saves the best checkpoints under:

```
outputs/checkpoints/FD00X/multitask_best.pt
```

### **7.2 Training the Baseline Model**

To train the **Baseline BiLSTM RUL model** on all subsets (FD001â€“FD004), run:

```bash
python -m src.training.train_baseline
```

This trains the baseline RUL-only model on all four NASA subsets and saves the best checkpoints under:

```
outputs/checkpoints/FD00X/baseline_best.pt
```
### **7.3 Model Evaluation (RUL, HI, Attention, and LLM Report for Multitask Model)**

To evaluate any trained model, run the evaluation script.  
Example for evaluating the **multitask** model on **FD001**:

```bash
python -m src.training.eval_rul --subset FD001 --model multitask
```

This evaluation generates the following components:

- **RMSE (Root Mean Squared Error)** â€“ Measures average prediction error magnitude.  
- **MAE (Mean Absolute Error)** â€“ Measures average absolute deviation from true RUL.  
- **NASA PHM Score** â€“ Official scoring metric from NASAâ€™s PHM challenge.  
- **RÂ² (Coefficient of Determination)** â€“ Measures how well predictions explain variance in true RUL.  
- **RUL Prediction Curves** â€“ Predicted vs. true RUL across time.  
- **Error Histograms** â€“ Distribution of prediction errors.  
- **Attention Curve** â€“ Temporal attention weights used during prediction.  
- **Health Index (HI) Sequence Visualization** â€“ Only for the multitask model.  
- **Sensor Degradation Plots** â€“ Sample degradation trends.  

#### **Multitask Model Only**
The multitask model additionally generates:

- **LLM Diagnostic Report (DOCX)** â€“ Produced using **DeepSeek-R1 via Ollama**, containing:
  - Sensor deviation analysis  
  - Engine health summary  
  - Detected degradation patterns  
  - Failure-mode insights  
  - Suggested maintenance recommendations  

All multitask evaluation results are saved under:

```
outputs/evaluation/FD00X/multitask/
```

---

### **Baseline Model Evaluation**

To evaluate the baseline **BiLSTM RUL-only model** on FD001:

```bash
python -m src.training.eval_rul --subset FD001 --model baseline
```

The baseline model generates:

- RMSE  
- MAE  
- NASA PHM Score  
- RÂ²  
- RUL prediction curve  
- Error histogram  
- Scatter: Predicted vs. True  
- Best & Worst sample plots  

âŒ *No HI prediction*  
âŒ *No attention weights*  
âŒ *No LLM diagnostic report*  

Baseline evaluation results are saved under:

```
outputs/evaluation/FD00X/baseline/
```

---
### **Files Included in Each Evaluation Folder**

- `metrics.txt`  
- `rul_prediction_curve.png`  
- `error_histogram.png`  
- `scatter_pred_vs_true.png`  
- `best_worst_samples.png`  
- `sensor_degradation_sample.png`  
- `rul_sequence_sample.png`  

**Multitask Only:**

- `attention_curve.png`  
- `hi_sequence_sample.png`  
- `llm_report.docx`

---

## **8. Limitations**

- **Health Index (HI)** is synthetic and not directly measured.
- The dataset used for training is entirely simulated, which may not capture the full complexity of real-world engine failures.
- **LLM-based output** quality depends heavily on prompt structure and training data quality.
- The system does not model **real-world sensor noise** or unexpected operational anomalies.

---

## **9. Future Work**

- Incorporate an **anomaly detection module** to improve early fault detection.
- Extend the model to use **Transformer-based architectures** to enhance sequential data modeling.
- Expand **LLM diagnostic capabilities** to handle batch-level analysis for multiple engines simultaneously.
- Perform **ablation studies** to evaluate the contribution of each attention layer.
- Deploy the system for **real-time inference** in production environments, integrating live engine data streams.

---

## **ğŸ“š References**
```
[1] S. Hochreiter and J. Schmidhuber, â€œLong short-term memory,â€ Neural Computation, vol. 9, no. 8, pp. 1735â€“1780, 1997.
[2] A. Vaswani et al., â€œAttention is all you need,â€ in Proc. 31st Int. Conf. Neural Information Processing Systems (NeurIPS), Long Beach, CA, USA, 2017, pp. 5998â€“6008.
[3] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimization,â€ arXiv preprint arXiv:1412.6980, 2015.
[4] A. Saxena, K. Goebel, D. Simon, and N. Eklund, â€œDamage propagation modeling for aircraft engine run-to-failure simulation,â€ in Proc. Int. Conf. Prognostics and Health Management (PHM), Denver, CO, USA, 2008, pp. 1â€“9.
[5] Y. Ren, C. Liu, and J. Zhang, â€œA survey of deep learning for remaining useful life prediction of aerospace engines,â€ Chinese Journal of Aeronautics, vol. 35, no. 8, pp. 1â€“23, 2022.
[6] S. Zheng, K. Ristovski, A. Farahat, and C. Gupta, â€œLong short-term memory network for remaining useful life estimation,â€ in Proc. IEEE Aerospace Conf., Big Sky, MT, USA, 2017, pp. 1â€“7.
[7] X. Li, Q. Ding, and J. Sun, â€œRemaining useful life estimation in prognostics using deep convolution neural networks,â€ IEEE Transactions on Industrial Electronics, vol. 65, no. 9, pp. 7290â€“7299, Sep. 2018.
[8] S. Bai, J. Z. Kolter, and V. Koltun, â€œAn empirical evaluation of generic convolutional and recurrent networks for sequence modeling,â€ arXiv preprint arXiv:1803.01271, 2018.
[9] C. Liu, X. Wang, and H. Li, â€œTCNâ€“Transformer hybrid model for turbofan engine remaining useful life prediction,â€ IEEE Transactions on Aerospace and Electronic Systems, vol. 59, no. 4, pp. 3567â€“3578, Aug. 2023.
[10] J. Li, H. Zhang, and P. Wang, â€œDual attention mechanism for remaining useful life prediction of turbofan engines,â€ in Proc. IEEE Int. Conf. Prognostics and Health Management (ICPHM), Detroit, MI, USA, 2021, pp. 1â€“6.
[11] Y. Chen, Y. Liu, and X. Zhang, â€œAttention-based BiLSTM for explainable remaining useful life prediction,â€ IEEE Transactions on Reliability, vol. 72, no. 1, pp. 345â€“356, Mar. 2023.
[12] Y. Zhang, Z. Wang, and C. Li, â€œMultitask learning for remaining useful life and health index prediction of turbofan engines,â€ IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1â€“12, 2022.
[13] Y. Wang, J. Liu, and Z. Chen, â€œPhysics-informed multitask learning for turbofan engine remaining useful life prediction,â€ Journal of Aerospace Information Systems, vol. 21, no. 3, pp. 189â€“202, Mar. 2024.
[14] Y. Liu, Z. Chen, and X. Wang, â€œPhysics-informed neural networks for turbofan engine remaining useful life prediction,â€ Journal of Computational Physics, vol. 462, p. 111185, 2022.
[15] H. Guo, Y. Zhang, and J. Liu, â€œDomain adaptation for cross-subset remaining useful life prediction of turbofan engines,â€ in Proc. AAAI Conf. Artificial Intelligence, vol. 37, no. 11, 2023, pp. 13245â€“13252.
[16] M. Zhao, P. Wang, and Y. Chen, â€œSensor selection for remaining useful life prediction using attention mechanism,â€ Sensors, vol. 23, no. 12, p. 5567, 2023.
[17] Y. Zhu, J. Li, and H. Huang, â€œReal-time remaining useful life prediction for turbofan engines using edge computing,â€ IEEE Internet of Things Journal, vol. 11, no. 5, pp. 8901â€“8910, Mar. 2024.
[18] F. Karim et al., â€œLSTM fully convolutional networks for time series classification,â€ IEEE Access, vol. 6, pp. 166â€“181, 2018.
[19] Y. LeCun, Y. Bengio, and G. Hinton, â€œDeep learning,â€ Nature, vol. 521, pp. 436â€“444, 2015.
[20] S. Zheng, A. Farahat, and C. Gupta, â€œRecurrent neural networks for remaining useful life estimation,â€ IEEE Aerospace and Electronic Systems Magazine, vol. 32, no. 11, pp. 6â€“15, 2017.
```